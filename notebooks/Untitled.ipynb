{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import numpy as np\n",
    "import roboscientist.equation.equation as rs_equation\n",
    "from experiments import run_experiment\n",
    "\n",
    "import roboscientist.equation.operators as rs_operators\n",
    "from scipy.special import lambertw\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.392859]\n",
      " [ 3.987968]\n",
      " [ 5.122973]\n",
      " [ 6.802601]\n",
      " [ 7.545995]\n",
      " [ 8.202622]\n",
      " [ 9.370277]\n",
      " [ 9.849015]\n",
      " [10.392087]\n",
      " [11.286035]\n",
      " [12.161154]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd468042310>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [\n",
    "9.370277,\n",
    "3.987968,\n",
    "7.545995,\n",
    "8.202622,\n",
    "9.849015,\n",
    "5.122973 ,\n",
    "10.392087,\n",
    "6.802601,\n",
    "2.392859,\n",
    "11.286035,\n",
    "12.161154\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "X = np.sort(X)\n",
    "\n",
    "X=np.array(X)[..., None]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(X)\n",
    "\n",
    "#noise = 1e-3 * np.random.rand(*X.shape)\n",
    "y  = [\n",
    "    3.899692,\n",
    "    4.848482,\n",
    "    4.637509,\n",
    "    4.888823,\n",
    "    4.871396,\n",
    "    4.866795,\n",
    "    4.894397,\n",
    "    4.883973,\n",
    "    4.882728,\n",
    "    4.885172,\n",
    "    4.895091\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "y=np.array(y)\n",
    "\n",
    "plt.plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.392859  ]\n",
      " [ 3.987968  ]\n",
      " [ 5.122973  ]\n",
      " [ 6.802601  ]\n",
      " [ 7.545995  ]\n",
      " [ 8.202622  ]\n",
      " [ 9.370277  ]\n",
      " [ 9.849015  ]\n",
      " [10.392087  ]\n",
      " [11.286035  ]\n",
      " [12.161154  ]\n",
      " [ 2.392859  ]\n",
      " [ 3.987968  ]\n",
      " [ 5.122973  ]\n",
      " [ 6.802601  ]\n",
      " [ 7.545995  ]\n",
      " [ 8.202622  ]\n",
      " [ 9.370277  ]\n",
      " [ 9.849015  ]\n",
      " [10.392087  ]\n",
      " [11.286035  ]\n",
      " [12.161154  ]\n",
      " [ 2.39120859]\n",
      " [ 3.98534749]\n",
      " [ 5.15587474]\n",
      " [ 6.78518896]\n",
      " [ 7.5505365 ]\n",
      " [ 8.19098804]\n",
      " [ 9.30653271]\n",
      " [ 9.83749889]\n",
      " [10.47433893]\n",
      " [11.29979316]\n",
      " [12.15085456]\n",
      " [ 2.39491183]\n",
      " [ 4.01863095]\n",
      " [ 5.12115043]\n",
      " [ 6.79729693]\n",
      " [ 7.5308837 ]\n",
      " [ 8.16734847]\n",
      " [ 9.33049928]\n",
      " [ 9.84657332]\n",
      " [10.39638323]\n",
      " [11.23059714]\n",
      " [12.18975401]\n",
      " [ 2.38305013]\n",
      " [ 3.98916205]\n",
      " [ 5.19115696]\n",
      " [ 6.77567189]\n",
      " [ 7.54841912]\n",
      " [ 8.18198763]\n",
      " [ 9.36131023]\n",
      " [ 9.75570368]\n",
      " [10.30232198]\n",
      " [11.30210355]\n",
      " [12.10390006]\n",
      " [ 2.41402232]\n",
      " [ 3.95088014]\n",
      " [ 5.12988464]\n",
      " [ 6.80445964]\n",
      " [ 7.48668898]\n",
      " [ 8.25932407]\n",
      " [ 9.36329679]\n",
      " [ 9.90668822]\n",
      " [10.55927982]\n",
      " [11.32445717]\n",
      " [12.14855515]\n",
      " [ 2.38526869]\n",
      " [ 3.99247051]\n",
      " [ 5.155176  ]\n",
      " [ 6.81208298]\n",
      " [ 7.62081776]\n",
      " [ 8.17665221]\n",
      " [ 9.28985092]\n",
      " [ 9.86071867]\n",
      " [10.50001257]\n",
      " [11.10485439]\n",
      " [12.10818504]\n",
      " [ 2.38763503]\n",
      " [ 3.9796237 ]\n",
      " [ 5.17643508]\n",
      " [ 6.85548873]\n",
      " [ 7.52980439]\n",
      " [ 8.32278552]\n",
      " [ 9.36114053]\n",
      " [ 9.86700084]\n",
      " [10.35179306]\n",
      " [11.1234202 ]\n",
      " [12.43524238]\n",
      " [ 2.45497067]\n",
      " [ 4.00686257]\n",
      " [ 5.08769525]\n",
      " [ 6.68021141]\n",
      " [ 7.62720968]\n",
      " [ 8.14110821]\n",
      " [ 9.47636952]\n",
      " [ 9.76928454]\n",
      " [10.52818825]\n",
      " [11.15426823]\n",
      " [12.32174991]\n",
      " [ 2.43778983]\n",
      " [ 4.05264011]\n",
      " [ 5.02358922]\n",
      " [ 6.78985282]\n",
      " [ 7.54596788]\n",
      " [ 7.96040331]\n",
      " [ 9.29618225]\n",
      " [ 9.80377107]\n",
      " [10.30038563]\n",
      " [11.11937912]\n",
      " [12.0500121 ]\n",
      " [ 2.39201313]\n",
      " [ 4.00059373]\n",
      " [ 5.22868728]\n",
      " [ 6.63776425]\n",
      " [ 7.59271491]\n",
      " [ 8.17082335]\n",
      " [ 9.15424217]\n",
      " [ 9.64635938]\n",
      " [10.44799902]\n",
      " [11.29244871]\n",
      " [11.95258696]\n",
      " [ 2.35101595]\n",
      " [ 3.98725599]\n",
      " [ 4.99382042]\n",
      " [ 6.89816766]\n",
      " [ 7.34141654]\n",
      " [ 8.30574868]\n",
      " [ 9.57874759]\n",
      " [ 9.72340017]\n",
      " [10.5057806 ]\n",
      " [11.28010716]\n",
      " [12.01565074]\n",
      " [ 2.43262626]\n",
      " [ 3.96659708]\n",
      " [ 5.26472016]\n",
      " [ 6.8094568 ]\n",
      " [ 7.54969891]\n",
      " [ 8.17972244]\n",
      " [ 9.4907109 ]\n",
      " [ 9.70371594]\n",
      " [10.20472993]\n",
      " [11.43502555]\n",
      " [12.27535897]\n",
      " [ 2.40289118]\n",
      " [ 4.08227414]\n",
      " [ 4.99034471]\n",
      " [ 6.89286432]\n",
      " [ 7.50034003]\n",
      " [ 8.2662282 ]\n",
      " [ 9.19510162]\n",
      " [ 9.75800136]\n",
      " [10.46375332]\n",
      " [11.40410565]\n",
      " [11.99939773]\n",
      " [ 2.40560236]\n",
      " [ 4.03642696]\n",
      " [ 5.06015676]\n",
      " [ 6.83322424]\n",
      " [ 7.54284517]\n",
      " [ 8.017425  ]\n",
      " [ 9.17540788]\n",
      " [ 9.71323222]\n",
      " [10.39779963]\n",
      " [11.02167212]\n",
      " [12.15274086]\n",
      " [ 2.38275245]\n",
      " [ 3.95240468]\n",
      " [ 5.28104572]\n",
      " [ 6.89472622]\n",
      " [ 7.55565978]\n",
      " [ 8.30384002]\n",
      " [ 9.41185248]\n",
      " [ 9.6862639 ]\n",
      " [10.43009689]\n",
      " [11.12972208]\n",
      " [12.70776652]\n",
      " [ 2.35561108]\n",
      " [ 3.97076588]\n",
      " [ 5.18341529]\n",
      " [ 6.92140736]\n",
      " [ 7.59156189]\n",
      " [ 8.27494751]\n",
      " [ 9.47812838]\n",
      " [ 9.84963952]\n",
      " [10.42080272]\n",
      " [11.42979546]\n",
      " [12.06463894]\n",
      " [ 2.42742892]\n",
      " [ 4.0178801 ]\n",
      " [ 5.20903739]\n",
      " [ 6.73388135]\n",
      " [ 7.48026631]\n",
      " [ 8.22582211]\n",
      " [ 9.35626385]\n",
      " [ 9.59331807]\n",
      " [10.36845624]\n",
      " [11.14421221]\n",
      " [11.95459655]\n",
      " [ 2.36839949]\n",
      " [ 3.9770414 ]\n",
      " [ 5.24565131]\n",
      " [ 6.71555371]\n",
      " [ 7.6755744 ]\n",
      " [ 8.11803179]\n",
      " [ 9.20013929]\n",
      " [ 9.90694249]\n",
      " [10.59733861]\n",
      " [11.45406253]\n",
      " [12.11801173]\n",
      " [ 2.40908996]\n",
      " [ 4.18137698]\n",
      " [ 5.03431885]\n",
      " [ 6.94591561]\n",
      " [ 7.62388494]\n",
      " [ 8.02193952]\n",
      " [ 9.20757658]\n",
      " [ 9.71484829]\n",
      " [10.16982637]\n",
      " [11.44993525]\n",
      " [12.22475685]\n",
      " [ 2.36402374]\n",
      " [ 4.02428831]\n",
      " [ 5.30805329]\n",
      " [ 6.79511427]\n",
      " [ 7.54281566]\n",
      " [ 8.157352  ]\n",
      " [ 9.58273492]\n",
      " [ 9.91467316]\n",
      " [10.49869132]\n",
      " [11.25218912]\n",
      " [12.1822471 ]\n",
      " [ 2.42541922]\n",
      " [ 3.92522448]\n",
      " [ 5.09073668]\n",
      " [ 6.90831306]\n",
      " [ 7.37582033]\n",
      " [ 8.10067303]\n",
      " [ 9.29553741]\n",
      " [ 9.58148054]\n",
      " [10.50666483]\n",
      " [11.4034057 ]\n",
      " [12.13745491]\n",
      " [ 2.37169662]\n",
      " [ 3.88501368]\n",
      " [ 5.35447705]\n",
      " [ 6.74239517]\n",
      " [ 7.53401277]\n",
      " [ 7.99584997]\n",
      " [ 9.38828962]\n",
      " [ 9.84436766]\n",
      " [10.27132785]\n",
      " [11.10972984]\n",
      " [11.98558609]\n",
      " [ 2.4256922 ]\n",
      " [ 3.90260307]\n",
      " [ 5.21038542]\n",
      " [ 6.84820264]\n",
      " [ 7.70026788]\n",
      " [ 8.4598592 ]\n",
      " [ 9.26948117]\n",
      " [ 9.7508049 ]\n",
      " [10.12268228]\n",
      " [11.12174068]\n",
      " [12.61591438]\n",
      " [ 2.44210221]\n",
      " [ 4.04345033]\n",
      " [ 5.03122207]\n",
      " [ 6.62983567]\n",
      " [ 7.80939518]\n",
      " [ 8.09963906]\n",
      " [ 9.46827697]\n",
      " [ 9.85766766]\n",
      " [10.60413272]\n",
      " [11.11197576]\n",
      " [12.31917765]\n",
      " [ 2.42365625]\n",
      " [ 4.01440785]\n",
      " [ 5.10712828]\n",
      " [ 6.57820399]\n",
      " [ 7.61273187]\n",
      " [ 8.17844853]\n",
      " [ 9.5852737 ]\n",
      " [ 9.84722948]\n",
      " [10.4302425 ]\n",
      " [11.10124827]\n",
      " [11.79982363]\n",
      " [ 2.37049775]\n",
      " [ 3.99297351]\n",
      " [ 5.27706363]\n",
      " [ 6.64787222]\n",
      " [ 7.43085079]\n",
      " [ 8.39360017]\n",
      " [ 9.40429707]\n",
      " [ 9.65002392]\n",
      " [10.66777662]\n",
      " [11.46056187]\n",
      " [11.83343423]\n",
      " [ 2.39564319]\n",
      " [ 3.88994737]\n",
      " [ 5.1462146 ]\n",
      " [ 6.80789609]\n",
      " [ 7.4718742 ]\n",
      " [ 8.43833379]\n",
      " [ 9.30114701]\n",
      " [ 9.63740577]\n",
      " [10.35751558]\n",
      " [11.15826873]\n",
      " [12.02990467]\n",
      " [ 2.46500184]\n",
      " [ 3.89051902]\n",
      " [ 5.27120643]\n",
      " [ 6.98810332]\n",
      " [ 7.42485159]\n",
      " [ 8.06677176]\n",
      " [ 9.55370333]\n",
      " [ 9.62798851]\n",
      " [10.23079448]\n",
      " [11.31640671]\n",
      " [12.09884996]\n",
      " [ 2.43786207]\n",
      " [ 4.07860088]\n",
      " [ 4.92447436]\n",
      " [ 6.94912211]\n",
      " [ 7.55980361]\n",
      " [ 8.05186864]\n",
      " [ 9.27394697]\n",
      " [ 9.71818902]\n",
      " [10.57279969]\n",
      " [11.56536531]\n",
      " [11.48100563]\n",
      " [ 2.36668412]\n",
      " [ 4.00777111]\n",
      " [ 4.88095246]\n",
      " [ 6.83563369]\n",
      " [ 7.36195616]\n",
      " [ 7.99650019]\n",
      " [ 9.21561212]\n",
      " [ 9.99441241]\n",
      " [10.59064138]\n",
      " [10.83718847]\n",
      " [12.02830209]\n",
      " [ 2.34901545]\n",
      " [ 3.83568786]\n",
      " [ 5.28410064]\n",
      " [ 6.88087623]\n",
      " [ 7.54546473]\n",
      " [ 8.28096853]\n",
      " [ 9.40620546]\n",
      " [ 9.6006286 ]\n",
      " [10.32508735]\n",
      " [10.98609783]\n",
      " [13.11278644]]\n",
      "[3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973\n",
      " 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396\n",
      " 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482\n",
      " 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172\n",
      " 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397\n",
      " 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823\n",
      " 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692\n",
      " 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728\n",
      " 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795\n",
      " 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509\n",
      " 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091\n",
      " 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973\n",
      " 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396\n",
      " 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482\n",
      " 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172\n",
      " 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397\n",
      " 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823\n",
      " 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692\n",
      " 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728\n",
      " 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795\n",
      " 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509\n",
      " 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091\n",
      " 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973\n",
      " 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396\n",
      " 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482\n",
      " 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172\n",
      " 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397\n",
      " 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823\n",
      " 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692\n",
      " 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728\n",
      " 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795\n",
      " 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509\n",
      " 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091\n",
      " 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973\n",
      " 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396\n",
      " 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482\n",
      " 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172\n",
      " 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397\n",
      " 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823\n",
      " 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692\n",
      " 4.848482 4.637509 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728\n",
      " 4.885172 4.895091 3.899692 4.848482 4.637509 4.888823 4.871396 4.866795\n",
      " 4.894397 4.883973 4.882728 4.885172 4.895091 3.899692 4.848482 4.637509\n",
      " 4.888823 4.871396 4.866795 4.894397 4.883973 4.882728 4.885172 4.895091]\n"
     ]
    }
   ],
   "source": [
    "for i in range (5):\n",
    "    noise = np.random.normal(X,0.004 * i * X)\n",
    "\n",
    "#print(noise)\n",
    "    X = np.append(X,noise)\n",
    "\n",
    "    y = np.append(y, y)\n",
    "\n",
    "X=np.array(X)[..., None]\n",
    "print(X)\n",
    "\n",
    "\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsprings(list_of_tokens, idx):\n",
    "    if list_of_tokens[idx] in rs_operators.OPERATORS:\n",
    "        open_nodes = rs_operators.OPERATORS[list_of_tokens[idx]].arity\n",
    "    else:\n",
    "        open_nodes = 0\n",
    "    traversal = []\n",
    "    for i, token in enumerate(list_of_tokens[idx + 1:]):\n",
    "        if open_nodes == 0:\n",
    "            break\n",
    "        traversal.append(token)\n",
    "        if token in rs_operators.OPERATORS:\n",
    "            operator = rs_operators.OPERATORS[token]\n",
    "            open_nodes += operator.arity - 1\n",
    "        else:\n",
    "            open_nodes -= 1\n",
    "\n",
    "    return traversal\n",
    "\n",
    "def predicate(list_of_tokens, chek = False):\n",
    "\n",
    "        \n",
    "#    if list_of_tokens[0] == 'sub':\n",
    " #       left, right = get_sub_trees_of_binary(list_of_tokens, 0)\n",
    "  #      if right not in rs_operators.FLOAT_CONST or lef !='mul':\n",
    "   #             return False\n",
    "\n",
    "  #if  'X**-2' not in list_of_tokens:\n",
    "   #   return False\n",
    "    \n",
    "    \n",
    "#    if 'cos**2' not in list_of_tokens and 'sin**2' not in list_of_tokens :\n",
    " #       return False\n",
    "    \n",
    "\n",
    "    for i, token in enumerate(list_of_tokens):\n",
    "        offsprings = get_offsprings(list_of_tokens, i)      \n",
    "        if token == 'sin**2' or token == 'cos**2'  or token == 'exp' or token == 'log' or token == 'log**2' or token == 'X**-2' or token == 'OSCL' or token == 'OSCE':\n",
    "            if 'sin**2' in offsprings or 'cos**2' in offsprings or 'exp' in offsprings or 'log' in offsprings or 'log**2' in offsprings or 'X**-2' in offsprings or 'OSCL' in offsprings or 'OSCE' in offsprings:\n",
    "                return False\n",
    "         \n",
    "            \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:101plmi0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 386496... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">Mo_S2_Mo_S_d_15p_nise_add_sub_mul_div_exp_pow2_log_cos_sin_100</strong>: <a href=\"https://wandb.ai/mikelazarev/segvae/runs/101plmi0\" target=\"_blank\">https://wandb.ai/mikelazarev/segvae/runs/101plmi0</a><br/>\n",
       "Find logs at: <code>../logs/wandb/run-20220304_112634-101plmi0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:101plmi0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/mikelazarev/segvae/runs/myiqqs3o\" target=\"_blank\">Mo_S2_Mo_S_d_15p_nise_add_sub_mul_div_exp_pow2_log_cos_sin_100</a></strong> to <a href=\"https://wandb.ai/mikelazarev/segvae\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== START PRETRAIN =====\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 21.060, rec loss: 20.986, kl: 0.149\n",
      "\t[validation] loss: 17.564, rec loss: 17.552, kl: 0.025\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 18.549, rec loss: 18.535, kl: 0.027\n",
      "\t[validation] loss: 16.806, rec loss: 16.796, kl: 0.020\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 18.013, rec loss: 17.997, kl: 0.033\n",
      "\t[validation] loss: 16.528, rec loss: 16.508, kl: 0.039\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.848, rec loss: 17.807, kl: 0.082\n",
      "\t[validation] loss: 16.458, rec loss: 16.392, kl: 0.131\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.749, rec loss: 17.656, kl: 0.186\n",
      "\t[validation] loss: 16.295, rec loss: 16.179, kl: 0.232\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.667, rec loss: 17.519, kl: 0.297\n",
      "\t[validation] loss: 16.241, rec loss: 16.058, kl: 0.367\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.598, rec loss: 17.393, kl: 0.409\n",
      "\t[validation] loss: 16.226, rec loss: 15.984, kl: 0.484\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.557, rec loss: 17.304, kl: 0.507\n",
      "\t[validation] loss: 16.201, rec loss: 15.904, kl: 0.595\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.517, rec loss: 17.233, kl: 0.569\n",
      "\t[validation] loss: 16.170, rec loss: 15.848, kl: 0.645\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.493, rec loss: 17.173, kl: 0.639\n",
      "\t[validation] loss: 16.166, rec loss: 15.818, kl: 0.696\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.453, rec loss: 17.096, kl: 0.714\n",
      "\t[validation] loss: 16.114, rec loss: 15.719, kl: 0.791\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.432, rec loss: 17.025, kl: 0.814\n",
      "\t[validation] loss: 16.111, rec loss: 15.651, kl: 0.919\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.393, rec loss: 16.945, kl: 0.895\n",
      "\t[validation] loss: 16.075, rec loss: 15.585, kl: 0.981\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.386, rec loss: 16.912, kl: 0.947\n",
      "\t[validation] loss: 16.037, rec loss: 15.517, kl: 1.041\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.355, rec loss: 16.862, kl: 0.985\n",
      "\t[validation] loss: 16.066, rec loss: 15.486, kl: 1.159\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.339, rec loss: 16.817, kl: 1.046\n",
      "\t[validation] loss: 16.015, rec loss: 15.438, kl: 1.155\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.303, rec loss: 16.758, kl: 1.090\n",
      "\t[validation] loss: 16.044, rec loss: 15.418, kl: 1.251\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.252, rec loss: 16.630, kl: 1.243\n",
      "\t[validation] loss: 16.023, rec loss: 15.265, kl: 1.517\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.178, rec loss: 16.450, kl: 1.456\n",
      "\t[validation] loss: 15.814, rec loss: 15.042, kl: 1.546\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.127, rec loss: 16.329, kl: 1.596\n",
      "\t[validation] loss: 15.844, rec loss: 15.026, kl: 1.636\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.080, rec loss: 16.220, kl: 1.720\n",
      "\t[validation] loss: 15.769, rec loss: 14.773, kl: 1.992\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.050, rec loss: 16.147, kl: 1.805\n",
      "\t[validation] loss: 15.786, rec loss: 14.835, kl: 1.900\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 17.008, rec loss: 16.052, kl: 1.912\n",
      "\t[validation] loss: 15.664, rec loss: 14.685, kl: 1.957\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.966, rec loss: 15.972, kl: 1.989\n",
      "\t[validation] loss: 15.680, rec loss: 14.598, kl: 2.163\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.929, rec loss: 15.875, kl: 2.107\n",
      "\t[validation] loss: 15.674, rec loss: 14.619, kl: 2.110\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.891, rec loss: 15.785, kl: 2.210\n",
      "\t[validation] loss: 15.549, rec loss: 14.407, kl: 2.284\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.816, rec loss: 15.627, kl: 2.377\n",
      "\t[validation] loss: 15.496, rec loss: 14.241, kl: 2.510\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.757, rec loss: 15.453, kl: 2.607\n",
      "\t[validation] loss: 15.403, rec loss: 14.092, kl: 2.622\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.681, rec loss: 15.281, kl: 2.800\n",
      "\t[validation] loss: 15.513, rec loss: 14.065, kl: 2.896\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.586, rec loss: 15.068, kl: 3.037\n",
      "\t[validation] loss: 15.220, rec loss: 13.662, kl: 3.115\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.499, rec loss: 14.847, kl: 3.305\n",
      "\t[validation] loss: 15.113, rec loss: 13.417, kl: 3.392\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.390, rec loss: 14.604, kl: 3.572\n",
      "\t[validation] loss: 15.001, rec loss: 13.144, kl: 3.715\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.333, rec loss: 14.450, kl: 3.767\n",
      "\t[validation] loss: 14.892, rec loss: 12.941, kl: 3.902\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.250, rec loss: 14.280, kl: 3.940\n",
      "\t[validation] loss: 14.870, rec loss: 12.885, kl: 3.970\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.229, rec loss: 14.193, kl: 4.072\n",
      "\t[validation] loss: 14.879, rec loss: 12.846, kl: 4.068\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.148, rec loss: 14.037, kl: 4.222\n",
      "\t[validation] loss: 14.794, rec loss: 12.670, kl: 4.249\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.083, rec loss: 13.899, kl: 4.367\n",
      "\t[validation] loss: 14.670, rec loss: 12.463, kl: 4.414\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 16.082, rec loss: 13.862, kl: 4.440\n",
      "\t[validation] loss: 14.656, rec loss: 12.434, kl: 4.442\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.991, rec loss: 13.701, kl: 4.579\n",
      "\t[validation] loss: 14.918, rec loss: 12.520, kl: 4.796\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.997, rec loss: 13.654, kl: 4.688\n",
      "\t[validation] loss: 14.626, rec loss: 12.200, kl: 4.853\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.949, rec loss: 13.569, kl: 4.761\n",
      "\t[validation] loss: 14.558, rec loss: 12.125, kl: 4.864\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.944, rec loss: 13.541, kl: 4.807\n",
      "\t[validation] loss: 14.568, rec loss: 12.120, kl: 4.897\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.883, rec loss: 13.431, kl: 4.904\n",
      "\t[validation] loss: 14.518, rec loss: 12.001, kl: 5.036\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.833, rec loss: 13.329, kl: 5.008\n",
      "\t[validation] loss: 14.591, rec loss: 12.126, kl: 4.930\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.850, rec loss: 13.323, kl: 5.054\n",
      "\t[validation] loss: 14.441, rec loss: 11.852, kl: 5.177\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.801, rec loss: 13.238, kl: 5.126\n",
      "\t[validation] loss: 14.457, rec loss: 11.837, kl: 5.241\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.790, rec loss: 13.205, kl: 5.171\n",
      "\t[validation] loss: 14.367, rec loss: 11.728, kl: 5.277\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.739, rec loss: 13.109, kl: 5.260\n",
      "\t[validation] loss: 14.380, rec loss: 11.721, kl: 5.318\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.705, rec loss: 13.048, kl: 5.314\n",
      "\t[validation] loss: 14.366, rec loss: 11.680, kl: 5.373\n",
      "\t[training] batches count: 196\n",
      "\t[training] loss: 15.726, rec loss: 13.049, kl: 5.355\n",
      "\t[validation] loss: 14.287, rec loss: 11.574, kl: 5.427\n",
      "===== END PRETRAIN =====\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 16.057, rec loss: 13.643, kl: 4.827\n",
      "\t[validation] loss: 16.678, rec loss: 14.512, kl: 4.331\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.542, rec loss: 15.074, kl: 4.936\n",
      "\t[validation] loss: 16.864, rec loss: 14.132, kl: 5.463\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.029, rec loss: 13.403, kl: 5.252\n",
      "\t[validation] loss: 15.642, rec loss: 13.146, kl: 4.993\n",
      "\t[training] batches count: 3\n",
      "\t[training] loss: 17.850, rec loss: 15.134, kl: 5.433\n",
      "\t[validation] loss: 20.862, rec loss: 17.876, kl: 5.972\n",
      "\t[training] batches count: 3\n",
      "\t[training] loss: 18.655, rec loss: 15.804, kl: 5.703\n",
      "\t[validation] loss: 17.982, rec loss: 15.182, kl: 5.600\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.788, rec loss: 14.260, kl: 5.056\n",
      "\t[validation] loss: 16.287, rec loss: 13.768, kl: 5.038\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.377, rec loss: 14.813, kl: 5.128\n",
      "\t[validation] loss: 17.104, rec loss: 14.551, kl: 5.107\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.976, rec loss: 14.477, kl: 4.999\n",
      "\t[validation] loss: 16.802, rec loss: 14.300, kl: 5.005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.207, rec loss: 14.728, kl: 4.960\n",
      "\t[validation] loss: 16.731, rec loss: 14.203, kl: 5.055\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.705, rec loss: 14.226, kl: 4.959\n",
      "\t[validation] loss: 16.196, rec loss: 13.676, kl: 5.041\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.613, rec loss: 14.123, kl: 4.980\n",
      "\t[validation] loss: 16.187, rec loss: 13.662, kl: 5.050\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.353, rec loss: 13.887, kl: 4.932\n",
      "\t[validation] loss: 16.370, rec loss: 13.923, kl: 4.893\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.328, rec loss: 14.822, kl: 5.013\n",
      "\t[validation] loss: 16.963, rec loss: 14.378, kl: 5.171\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.232, rec loss: 14.678, kl: 5.109\n",
      "\t[validation] loss: 16.784, rec loss: 14.260, kl: 5.048\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.449, rec loss: 14.918, kl: 5.062\n",
      "\t[validation] loss: 16.939, rec loss: 14.313, kl: 5.253\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.060, rec loss: 14.496, kl: 5.128\n",
      "\t[validation] loss: 16.629, rec loss: 14.143, kl: 4.973\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.118, rec loss: 14.662, kl: 4.912\n",
      "\t[validation] loss: 16.316, rec loss: 13.833, kl: 4.966\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.555, rec loss: 14.087, kl: 4.936\n",
      "\t[validation] loss: 16.239, rec loss: 13.746, kl: 4.987\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.080, rec loss: 13.642, kl: 4.878\n",
      "\t[validation] loss: 15.662, rec loss: 13.258, kl: 4.808\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.867, rec loss: 12.531, kl: 4.673\n",
      "\t[validation] loss: 14.672, rec loss: 12.280, kl: 4.784\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.330, rec loss: 12.027, kl: 4.605\n",
      "\t[validation] loss: 13.892, rec loss: 11.583, kl: 4.617\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.090, rec loss: 11.793, kl: 4.593\n",
      "\t[validation] loss: 13.568, rec loss: 11.227, kl: 4.681\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.063, rec loss: 11.723, kl: 4.680\n",
      "\t[validation] loss: 13.638, rec loss: 11.383, kl: 4.511\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 13.821, rec loss: 11.552, kl: 4.538\n",
      "\t[validation] loss: 13.837, rec loss: 11.549, kl: 4.575\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.221, rec loss: 11.903, kl: 4.636\n",
      "\t[validation] loss: 14.181, rec loss: 11.961, kl: 4.439\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.721, rec loss: 12.505, kl: 4.433\n",
      "\t[validation] loss: 15.197, rec loss: 12.860, kl: 4.675\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 15.019, rec loss: 12.729, kl: 4.579\n",
      "\t[validation] loss: 13.983, rec loss: 11.748, kl: 4.470\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.435, rec loss: 12.178, kl: 4.515\n",
      "\t[validation] loss: 13.763, rec loss: 11.453, kl: 4.620\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.241, rec loss: 11.938, kl: 4.608\n",
      "\t[validation] loss: 13.815, rec loss: 11.492, kl: 4.646\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.673, rec loss: 12.370, kl: 4.607\n",
      "\t[validation] loss: 14.108, rec loss: 11.790, kl: 4.635\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 15.160, rec loss: 12.741, kl: 4.839\n",
      "\t[validation] loss: 14.672, rec loss: 12.280, kl: 4.782\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.584, rec loss: 12.271, kl: 4.624\n",
      "\t[validation] loss: 13.838, rec loss: 11.491, kl: 4.694\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.747, rec loss: 12.368, kl: 4.758\n",
      "\t[validation] loss: 15.254, rec loss: 12.938, kl: 4.633\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.969, rec loss: 12.659, kl: 4.620\n",
      "\t[validation] loss: 14.868, rec loss: 12.460, kl: 4.816\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.764, rec loss: 12.384, kl: 4.761\n",
      "\t[validation] loss: 15.177, rec loss: 12.865, kl: 4.626\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.801, rec loss: 12.509, kl: 4.583\n",
      "\t[validation] loss: 14.374, rec loss: 12.070, kl: 4.608\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.525, rec loss: 12.275, kl: 4.500\n",
      "\t[validation] loss: 13.887, rec loss: 11.711, kl: 4.352\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.390, rec loss: 12.237, kl: 4.305\n",
      "\t[validation] loss: 13.629, rec loss: 11.446, kl: 4.365\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.398, rec loss: 12.234, kl: 4.329\n",
      "\t[validation] loss: 14.007, rec loss: 11.832, kl: 4.349\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.082, rec loss: 11.897, kl: 4.371\n",
      "\t[validation] loss: 13.681, rec loss: 11.508, kl: 4.347\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.123, rec loss: 11.964, kl: 4.318\n",
      "\t[validation] loss: 13.931, rec loss: 11.740, kl: 4.383\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.667, rec loss: 12.470, kl: 4.396\n",
      "\t[validation] loss: 14.235, rec loss: 12.043, kl: 4.384\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.104, rec loss: 11.996, kl: 4.215\n",
      "\t[validation] loss: 13.914, rec loss: 11.829, kl: 4.169\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 17.505, rec loss: 15.355, kl: 4.300\n",
      "\t[validation] loss: 17.954, rec loss: 15.128, kl: 5.651\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 15.884, rec loss: 13.407, kl: 4.953\n",
      "\t[validation] loss: 15.288, rec loss: 12.943, kl: 4.691\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 13.684, rec loss: 11.569, kl: 4.230\n",
      "\t[validation] loss: 12.425, rec loss: 10.420, kl: 4.008\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.959, rec loss: 10.954, kl: 4.009\n",
      "\t[validation] loss: 12.591, rec loss: 10.607, kl: 3.967\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 13.188, rec loss: 11.202, kl: 3.971\n",
      "\t[validation] loss: 12.571, rec loss: 10.594, kl: 3.955\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 13.355, rec loss: 11.388, kl: 3.934\n",
      "\t[validation] loss: 12.667, rec loss: 10.712, kl: 3.911\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.925, rec loss: 10.990, kl: 3.870\n",
      "\t[validation] loss: 12.576, rec loss: 10.655, kl: 3.841\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.986, rec loss: 11.072, kl: 3.828\n",
      "\t[validation] loss: 12.636, rec loss: 10.737, kl: 3.799\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.804, rec loss: 10.935, kl: 3.738\n",
      "\t[validation] loss: 12.464, rec loss: 10.608, kl: 3.712\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.652, rec loss: 10.817, kl: 3.669\n",
      "\t[validation] loss: 12.355, rec loss: 10.525, kl: 3.660\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.436, rec loss: 10.624, kl: 3.624\n",
      "\t[validation] loss: 12.120, rec loss: 10.313, kl: 3.615\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.364, rec loss: 10.560, kl: 3.606\n",
      "\t[validation] loss: 12.116, rec loss: 10.319, kl: 3.594\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.233, rec loss: 10.455, kl: 3.556\n",
      "\t[validation] loss: 11.984, rec loss: 10.205, kl: 3.557\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.114, rec loss: 10.340, kl: 3.548\n",
      "\t[validation] loss: 11.854, rec loss: 10.087, kl: 3.535\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.189, rec loss: 10.435, kl: 3.509\n",
      "\t[validation] loss: 11.747, rec loss: 9.994, kl: 3.506\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.098, rec loss: 10.345, kl: 3.505\n",
      "\t[validation] loss: 11.851, rec loss: 10.096, kl: 3.512\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.900, rec loss: 10.155, kl: 3.489\n",
      "\t[validation] loss: 11.644, rec loss: 9.885, kl: 3.517\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 16.257, rec loss: 14.307, kl: 3.900\n",
      "\t[validation] loss: 14.210, rec loss: 12.308, kl: 3.806\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.660, rec loss: 12.776, kl: 3.768\n",
      "\t[validation] loss: 14.198, rec loss: 12.352, kl: 3.692\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.294, rec loss: 12.478, kl: 3.634\n",
      "\t[validation] loss: 13.795, rec loss: 12.027, kl: 3.537\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 14.731, rec loss: 12.982, kl: 3.497\n",
      "\t[validation] loss: 14.081, rec loss: 12.322, kl: 3.516\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.097, rec loss: 10.525, kl: 3.144\n",
      "\t[validation] loss: 11.824, rec loss: 10.241, kl: 3.166\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.195, rec loss: 10.612, kl: 3.166\n",
      "\t[validation] loss: 11.934, rec loss: 10.317, kl: 3.236\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.523, rec loss: 10.898, kl: 3.248\n",
      "\t[validation] loss: 11.906, rec loss: 10.235, kl: 3.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.141, rec loss: 10.496, kl: 3.290\n",
      "\t[validation] loss: 11.933, rec loss: 10.272, kl: 3.323\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.231, rec loss: 10.596, kl: 3.271\n",
      "\t[validation] loss: 11.846, rec loss: 10.181, kl: 3.329\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.977, rec loss: 10.336, kl: 3.281\n",
      "\t[validation] loss: 11.680, rec loss: 10.033, kl: 3.294\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 12.186, rec loss: 10.568, kl: 3.236\n",
      "\t[validation] loss: 11.585, rec loss: 9.955, kl: 3.260\n",
      "\t[training] batches count: 2\n",
      "\t[training] loss: 13.680, rec loss: 11.925, kl: 3.512\n",
      "\t[validation] loss: 13.646, rec loss: 11.852, kl: 3.587\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.924, rec loss: 10.295, kl: 3.258\n",
      "\t[validation] loss: 11.542, rec loss: 9.912, kl: 3.260\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.621, rec loss: 10.002, kl: 3.238\n",
      "\t[validation] loss: 11.525, rec loss: 9.916, kl: 3.218\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.803, rec loss: 10.183, kl: 3.240\n",
      "\t[validation] loss: 11.332, rec loss: 9.700, kl: 3.265\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.650, rec loss: 10.002, kl: 3.296\n",
      "\t[validation] loss: 11.297, rec loss: 9.630, kl: 3.333\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.460, rec loss: 9.814, kl: 3.292\n",
      "\t[validation] loss: 11.176, rec loss: 9.515, kl: 3.322\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.285, rec loss: 9.637, kl: 3.295\n",
      "\t[validation] loss: 10.967, rec loss: 9.320, kl: 3.294\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.450, rec loss: 9.816, kl: 3.268\n",
      "\t[validation] loss: 10.992, rec loss: 9.346, kl: 3.292\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.325, rec loss: 9.708, kl: 3.235\n",
      "\t[validation] loss: 11.044, rec loss: 9.408, kl: 3.271\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.518, rec loss: 9.904, kl: 3.228\n",
      "\t[validation] loss: 11.168, rec loss: 9.553, kl: 3.231\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.610, rec loss: 9.996, kl: 3.229\n",
      "\t[validation] loss: 11.140, rec loss: 9.519, kl: 3.241\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.585, rec loss: 9.970, kl: 3.229\n",
      "\t[validation] loss: 11.196, rec loss: 9.569, kl: 3.254\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.529, rec loss: 9.908, kl: 3.242\n",
      "\t[validation] loss: 11.087, rec loss: 9.442, kl: 3.290\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.068, rec loss: 9.443, kl: 3.251\n",
      "\t[validation] loss: 10.845, rec loss: 9.209, kl: 3.271\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.036, rec loss: 9.394, kl: 3.283\n",
      "\t[validation] loss: 10.629, rec loss: 8.978, kl: 3.301\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.086, rec loss: 9.442, kl: 3.288\n",
      "\t[validation] loss: 10.620, rec loss: 8.969, kl: 3.301\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 10.950, rec loss: 9.322, kl: 3.257\n",
      "\t[validation] loss: 10.623, rec loss: 8.992, kl: 3.262\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.034, rec loss: 9.405, kl: 3.259\n",
      "\t[validation] loss: 10.782, rec loss: 9.145, kl: 3.272\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.023, rec loss: 9.396, kl: 3.255\n",
      "\t[validation] loss: 10.725, rec loss: 9.096, kl: 3.259\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.053, rec loss: 9.443, kl: 3.220\n",
      "\t[validation] loss: 10.756, rec loss: 9.125, kl: 3.264\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.013, rec loss: 9.393, kl: 3.240\n",
      "\t[validation] loss: 10.661, rec loss: 9.011, kl: 3.300\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.090, rec loss: 9.455, kl: 3.272\n",
      "\t[validation] loss: 10.711, rec loss: 9.066, kl: 3.290\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.236, rec loss: 9.606, kl: 3.260\n",
      "\t[validation] loss: 10.884, rec loss: 9.241, kl: 3.285\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.213, rec loss: 9.581, kl: 3.264\n",
      "\t[validation] loss: 10.798, rec loss: 9.144, kl: 3.307\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 11.191, rec loss: 9.536, kl: 3.311\n",
      "\t[validation] loss: 10.692, rec loss: 9.024, kl: 3.334\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 10.883, rec loss: 9.233, kl: 3.300\n",
      "\t[validation] loss: 10.686, rec loss: 9.028, kl: 3.316\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 10.971, rec loss: 9.319, kl: 3.304\n",
      "\t[validation] loss: 10.534, rec loss: 8.875, kl: 3.318\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 10.736, rec loss: 9.098, kl: 3.275\n",
      "\t[validation] loss: 10.558, rec loss: 8.918, kl: 3.281\n",
      "\t[training] batches count: 1\n",
      "\t[training] loss: 10.990, rec loss: 9.358, kl: 3.264\n",
      "\t[validation] loss: 10.708, rec loss: 9.064, kl: 3.289\n"
     ]
    }
   ],
   "source": [
    "solver = run_experiment(X, y,\n",
    "        functions=['add', 'sub', 'mul', 'div','pow2','exp','log','cos','sin'],\n",
    "        free_variables=['x1'],\n",
    "        wandb_proj='SEGVAE',\n",
    "        project_name='Mo_S2_Mo_S_d_15p_nise_add_sub_mul_div_exp_pow2_log_cos_sin_100',\n",
    "        constants=[],\n",
    "        float_constants=rs_operators.FLOAT_CONST,\n",
    "        #float_constants=None,\n",
    "        epochs=100,\n",
    "        n_formulas_to_sample=5000,\n",
    "        max_formula_length=12,\n",
    "        formula_predicate=predicate,\n",
    "        #true_formula=rs_equation.Equation(true_func),\n",
    "        latent=128,\n",
    "        lstm_hidden_dim=64,\n",
    "        device='cuda',\n",
    "        train_size=50000,\n",
    "        log_intermediate_steps=True,\n",
    "        pretrain_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw equation in prefix notation:  sub 5.0 sin**2 sub 6.0 div pow2 div 4.0 x1 x1\n",
      "Equation in traditional notation:  (5.0 - (sin**2((6.0 - (((4.0 / x1)^2) / x1)))))\n"
     ]
    }
   ],
   "source": [
    "best_idx = np.argmin(solver.stats.last_n_best_mses)\n",
    "equations = np.array(solver.stats.last_n_best_formulas)\n",
    "eq = rs_equation.Equation(equations[best_idx].split())\n",
    "\n",
    "print('Raw equation in prefix notation: ', equations[best_idx])\n",
    "print('Equation in traditional notation: ', eq.repr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=120)\n",
    "distance= np.linspace(min(X), max(X))\n",
    "\n",
    "#arg = 4.0 * (distance - 2.0)\n",
    "#plt.plot(arg)\n",
    "#print(arg)\n",
    "\n",
    "#def oscl(arg):\n",
    " #   return np.cos(arg) * np.cos(arg) * np.log(arg) * np.log(arg) / arg\n",
    "    \n",
    "#print(oscl(distance))\n",
    "\n",
    "#Energy = (5.0 - ((np.cos((((10.0 + 1.0) / distance) - 5.0)))**2 / (np.log(distance)**2)))\n",
    "\n",
    "\n",
    "Energy = (5.0 - ((np.sin((9.0 + distance)))**2 / (np.log(distance))**2))\n",
    "\n",
    "(4.0 + (cos**2(((2.0 + ((2.0 + ((9.0^2) / (x1^2))) / x1)) / 7.0))))\n",
    "#5.0 - ((OSCL((4.0 * (x1 - 2.0))))^2)\n",
    "\n",
    "print(distance)\n",
    "#plt.plot(distance, Energy)\n",
    "                 \n",
    "print(Energy)                \n",
    "plt.plot(distance,Energy)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_ylabel('Formation Energy per site (eV)'),\n",
    "ax.set_xlabel('Distance between defects')\n",
    "ax.set_title('Mo_S_deffects')\n",
    "        \n",
    "X1 = [\n",
    "9.370277,\n",
    "3.987968,\n",
    "7.545995,\n",
    "8.202622,\n",
    "9.849015,\n",
    "5.122973 ,\n",
    "10.392087,\n",
    "6.802601,\n",
    "2.392859,\n",
    "11.286035,\n",
    "12.161154\n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "X1 = np.sort(X1)\n",
    "        \n",
    "y1  = [\n",
    "    3.899692,\n",
    "    4.848482,\n",
    "    4.637509,\n",
    "    4.888823,\n",
    "    4.871396,\n",
    "    4.866795,\n",
    "    4.894397,\n",
    "    4.883973,\n",
    "    4.882728,\n",
    "    4.885172,\n",
    "    4.895091\n",
    "\n",
    "\n",
    "    ]  \n",
    "\n",
    "plt.plot(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
